---
id: 3
img: 2.png
title: 'Istio/envoy oauth2 connectivity failure due to happy_eyeballs'
desc: "Resolving a 'network unreachable' error with Azure Entra and Istio Envoy"
tags:
  - sre
  - istio
  - envoy
  - oauth2
date: '2024-08-12T04:00:00.000Z'
---

Recently, while trying to implement OAuth2 on an application in Istio, I was hitting the following error.

```
httpbin-c6b85f985-zslgh istio-proxy 2024-10-04T13:37:05.541213Z debug   envoy oauth2 external/envoy/source/extensions/filters/http/oauth2/oauth_client.cc:87    Oauth response code: 503   thread=28
httpbin-c6b85f985-zslgh istio-proxy 2024-10-04T13:37:05.541215Z debug   envoy oauth2 external/envoy/source/extensions/filters/http/oauth2/oauth_client.cc:88    Oauth response body: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: immediate connect error: Network is unreachable        thread=28
```

I was using Azure Entra ID as the provider so this error was confusing.

Adding to that, prior to using Envoy's [OAuth2 HTTP filter](https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/oauth2_filter), I was using the [oauth2-proxy](https://oauth2-proxy.github.io/oauth2-proxy/) which had no issues connecting to `https://login.microsoftonline.com`.

Something was off in the Istio sidecar where the request was originating from.

A quick test with `dig`, `nc` and `curl` confirmed my suspicions and indicated that the Azure Entra ID authorize endpoint `https://login.microsoftonline.com/<AZURE-TENANT-ID>/oauth2/v2.0/authorize` was indeed reachable from the same node and pod where the problematic application was residing.

After spending two days trying to figure this out, I learned about `happy_eyeballs`.

**happy_eyeballs**  
_"Happy Eyeballs (also called Fast Fallback) is an algorithm published by the IETF which can make dual-stack applications (those that understand both IPv4 and IPv6) more responsive to users by attempting to connect using both IPv4 and IPv6 at the same time (preferring IPv6), thus avoiding the usual problems faced by users with imperfect IPv6 connections or setups."_ - Wikipedia<sup>[1]</sup>

Turns out the Istio sidecar on the application was resolving `https://login.microsoftonline.com` to IPv6. This was confirmed by enabling `happy_eyeballs` `debug` logging on the sidecar pod.

```bash
# Check log level
> istioctl proxy-config log <APPLICATION_SIDECAR> | grep happy_eyeballs 
  happy_eyeballs: warning

# enable `debug` just for happy_eyeballs
> istioctl proxy-config log <APPLICATION_SIDECAR> --level "happy_eyeballs:debug" 

# enable `debug` for all
> istioctl proxy-config log <APPLICATION_SIDECAR> --level "debug"
```

And then the sidecar logs showing connectivity attempts to IPv6:
```
2024-08-09T09:04:46.347021Z     debug   envoy happy_eyeballs external/envoy/source/common/network/happy_eyeballs_connection_impl.cc:33  C[985] address=[2606:4700::6810:a016]:443       thread=23
```

Looking through Envoy's [Cluster docs](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/cluster/v3/cluster.proto.html), I found `dns_lookup_family`.

Setting this in the Cluster config patch resolved the issue:
```diff
  - applyTo: CLUSTER
    match:
      cluster:
        service: oauth
    patch:
      operation: ADD
      value:
        name: oauth
        connect_timeout: 10s
        lb_policy: ROUND_ROBIN
        load_assignment:
          cluster_name: oauth
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: login.microsoftonline.com
                    port_value: 443
        transport_socket:
          name: envoy.transport_sockets.tls
          typed_config:
            '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
            sni: login.microsoftonline.com
        type: LOGICAL_DNS
+       dns_lookup_family: V4_ONLY
```

## References
1. https://orhanergun.net/what-is-happy-eyeballs